{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "import re\n",
    "from scipy import stats\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_links_scrape():\n",
    "    # paper_link_CSS = 'ul > li > h4 > a:nth-child(1)'\n",
    "    session = webdriver.Chrome()\n",
    "    session.get('https://openreview.net/group?id=ICLR.cc/2019/Conference')\n",
    "    paper_links = set()\n",
    "    paper_link_CSS = '#all-submissions > ul > li > h4 > a:nth-child(1)'\n",
    "    prev_len = []\n",
    "    for i in range(100):\n",
    "        session.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        new_papers = set(session.find_elements_by_css_selector(paper_link_CSS))\n",
    "        new_papers = set([i.get_attribute('href') for i in new_papers])\n",
    "        paper_links = paper_links.union(new_papers)\n",
    "        time.sleep(1)\n",
    "        if len(prev_len) > 5 and len(paper_links) == prev_len[-5]:\n",
    "            break\n",
    "        prev_len.append(len(paper_links))\n",
    "    new_papers = set(session.find_elements_by_css_selector(paper_link_CSS))\n",
    "    new_papers = set([i.get_attribute('href') for i in new_papers])\n",
    "    paper_links = paper_links.union(new_papers)\n",
    "    return paper_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_links_load(filename):\n",
    "    return [i['url'] for i in json.load(open(filename))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_links = get_paper_links_scrape()\n",
    "# paper_links = get_paper_links_load('data/iclr2018.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "paper_links = list(paper_links)\n",
    "papers = {}\n",
    "print(len(paper_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = webdriver.Chrome()\n",
    "start_location = len(papers)\n",
    "\n",
    "idx = start_location\n",
    "while idx < len(paper_links):\n",
    "    link = paper_links[idx]\n",
    "    print(len(papers.items()), link)\n",
    "    data = {}\n",
    " \n",
    "    session.get(link)\n",
    "    time.sleep(2)\n",
    "\n",
    "    def update_non_comments():\n",
    "        title = session.find_element_by_css_selector('div.title_pdf_row > h2 > a:nth-child(1)').get_attribute('text')\n",
    "        abstract = session.find_element_by_css_selector('#content > div:nth-child(1) > div.note_contents > span.note_content_value').get_attribute('innerHTML')\n",
    "        \n",
    "        data['title'] = title\n",
    "        data['abstract'] = abstract\n",
    "        authors = session.find_element_by_css_selector('.signatures').find_elements_by_css_selector('a')\n",
    "        data['authors'] = [i.get_attribute('innerText') for i in authors]\n",
    "        data['emails'] = [i.get_attribute('data-original-title') for i in authors]\n",
    "        data['emails'] = list(set([i[i.find('@')+1:] for i in data['emails']]))\n",
    "        print(data['authors'], data['emails'])\n",
    "        papers[link] = data\n",
    "    \n",
    "    comment_CSS = '#note_children > div.note_with_children'\n",
    "    try:\n",
    "        WebDriverWait(session, 30).until(\n",
    "            lambda session: len(session.find_elements_by_css_selector(comment_CSS)) >=1)\n",
    "    except:\n",
    "        if len(session.find_elements_by_css_selector('.spinner')) == 1:\n",
    "            continue\n",
    "        elif session.current_url =='https://openreview.net/':\n",
    "            continue\n",
    "        elif '502' in session.title:\n",
    "            continue\n",
    "        elif '404' in session.title:\n",
    "            idx += 1\n",
    "            continue\n",
    "        else:\n",
    "            idx += 1\n",
    "            update_non_comments()\n",
    "            continue\n",
    "    comments = session.find_elements_by_css_selector(comment_CSS)\n",
    "    comment_vals = []\n",
    "    for comment in comments:\n",
    "        sections = comment.find_elements_by_css_selector('.note_contents')\n",
    "        section_vals = {}\n",
    "        for section in sections:\n",
    "            field = section.find_element_by_css_selector('.note_content_field')\n",
    "            field = field.get_attribute('innerHTML').strip()\n",
    "            content = section.find_element_by_css_selector('.note_content_value')\n",
    "            content = content.get_attribute('innerHTML').strip()\n",
    "            section_vals[field] = content\n",
    "        comment_vals.append(section_vals)\n",
    "    update_non_comments()\n",
    "    data['comments'] = comment_vals\n",
    "    \n",
    "    papers[link] = data \n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "scraped_papers = copy.deepcopy(papers)\n",
    "print(len(papers), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "papers = copy.deepcopy(scraped_papers)\n",
    "for paper in list(papers.values()):\n",
    "    if 'comments' not in paper:\n",
    "        continue\n",
    "    for idx in range(len(paper['comments'])):\n",
    "        comment = paper['comments'][idx]\n",
    "        comment = {k.strip(': '): v for k,v in comment.items()}\n",
    "        if 'Rating' in comment:\n",
    "            comment['Rating'] = re.search(r'^\\d+', comment['Rating']).group()\n",
    "        if 'Confidence' in comment:\n",
    "            comment['Confidence'] = re.search(r'^\\d+', comment['Confidence']).group()\n",
    "        else:\n",
    "            comment['Confidence'] = '4'\n",
    "        paper['comments'][idx] = comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "def generate_top_papers(papers):\n",
    "    overall_ratings = {}\n",
    "    for paper in list(papers.values()):\n",
    "        real_ratings = []\n",
    "        ratings = []\n",
    "        confidence = []\n",
    "        if 'comments' not in paper:\n",
    "            continue\n",
    "        for comment in paper['comments']:\n",
    "            if 'Rating' in comment:\n",
    "                real_ratings.append(int(comment['Rating']))\n",
    "                ratings.append(int(comment['Rating'])*int(comment['Confidence']))\n",
    "                confidence.append(int(comment['Confidence']))\n",
    "\n",
    "        if len(ratings) == 0:\n",
    "            continue\n",
    "    #     print(paper['title'])\n",
    "        overall_ratings[paper['title']] = (sum(ratings)/(len(ratings)*(sum(confidence)/len(confidence))), real_ratings, confidence)\n",
    "\n",
    "    print(len(overall_ratings))\n",
    "    all_ratings = sorted(list(overall_ratings.items()), key=lambda x:x[1][0], reverse=True)\n",
    "    for name, rating in all_ratings:\n",
    "        print(f'{\"%.2f\" % rating[0]},{name},{tuple(rating[1])},{tuple(rating[2])}')\n",
    "generate_top_papers(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "final_json = []\n",
    "review_scores = []\n",
    "confidence_scores = []\n",
    "for url, paper in papers.items():\n",
    "    result = {}\n",
    "    result['url'] = url\n",
    "    real_ratings = []\n",
    "    ratings = []\n",
    "    confidence = []\n",
    "    if 'comments' in paper:\n",
    "        for comment in paper['comments']:\n",
    "            if 'Rating' in comment and 'Confidence' in comment:\n",
    "                real_ratings.append(int(comment['Rating']))\n",
    "                ratings.append(int(comment['Rating'])*int(comment['Confidence']))\n",
    "                review_scores.append(int(comment['Rating']))\n",
    "                confidence_scores.append(int(comment['Confidence']))\n",
    "                confidence.append(int(comment['Confidence']))\n",
    "            elif 'Decision' in comment:\n",
    "                result['decision'] = comment['Decision']\n",
    "        result['comments'] = paper['comments']\n",
    "    result['ratings'] = real_ratings\n",
    "    if len(ratings) != 0:\n",
    "        result['rating'] = \"%.2f\" % (sum(ratings)/(len(ratings)*(sum(confidence)/len(confidence))))\n",
    "    else:\n",
    "        result['rating'] = ' N/A'\n",
    "    result['confidences'] = confidence\n",
    "    result['abstract'] = paper['abstract']\n",
    "    result['title'] = paper['title']\n",
    "    result['authors'] = paper['authors']\n",
    "    result['emails'] = paper['emails']\n",
    "    final_json.append(result)\n",
    "\n",
    "sorted_final = sorted(final_json, key=lambda x: x['rating'], reverse=True)\n",
    "final_json = []\n",
    "for idx, i in enumerate(sorted_final):\n",
    "    i['rank'] = idx+1\n",
    "    final_json.append(i)\n",
    "\n",
    "f = open('data.json', 'w')\n",
    "print(f.write(json.dumps(final_json, indent=2)))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a3100b822683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinal_json\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m' N/A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# review_dict = {k:0 for k in range(1, 11)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = open('data.json', 'r')\n",
    "final_json = json.loads(f.read())\n",
    "f.close()\n",
    "\n",
    "ratings = [float(i['rating']) for i in final_json if i['rating'] != ' N/A']\n",
    "ratings = sorted(ratings)\n",
    "print(sum(ratings)/len(ratings))\n",
    "print(ratings[len(ratings)*3//4])\n",
    "# review_dict = {k:0 for k in range(1, 11)}\n",
    "# for i in review_scores:\n",
    "#     review_dict[i] += 1\n",
    "# print(sum(review_scores)/len(review_scores))\n",
    "# print(review_dict)\n",
    "# confidence_dict = {k:0 for k in range(1, 6)}\n",
    "# for i in confidence_scores:\n",
    "#     confidence_dict[i] += 1\n",
    "# print(confidence_dict)\n",
    "def variance(arr, conf):\n",
    "    if len(arr) == 0:\n",
    "        return 0\n",
    "    return np.var(arr)\n",
    "\n",
    "#     mu = sum(arr)/len(arr)\n",
    "#     var = 0\n",
    "#     for idx in range(len(arr)):\n",
    "#         var += (arr[idx] - mu)**2\n",
    "#     )\n",
    "#     return var/len(arr)\n",
    "\n",
    "variance = [(i['title'], variance(i['ratings'], i['confidences'])) for i in final_json]\n",
    "\n",
    "variance = sorted(variance, key=lambda x:x[1], reverse=True)\n",
    "variance_dict = {k:v for k,v in variance}\n",
    "for title, v in variance[:10]:\n",
    "    print(f'{\"%.2f\" % v}: {title}')\n",
    "    \n",
    "entropies = [(i['title'], stats.entropy(i['ratings'])/len(i['ratings'])) for i in final_json]\n",
    "entropies = sorted(entropies, key=lambda x:x[1])\n",
    "print(entropies[:10])\n",
    "for idx, i in enumerate(final_json):\n",
    "    i['variance'] = float(\"%.2f\" % variance_dict[i['title']]**(1/2))\n",
    "    final_json[idx] = i\n",
    "    \n",
    "f = open('data.json', 'w')\n",
    "print(f.write(json.dumps(final_json, indent=2)))\n",
    "f.close()\n",
    "# print(final_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1331551\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
